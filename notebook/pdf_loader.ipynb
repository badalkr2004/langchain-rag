{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b73203db",
   "metadata": {},
   "source": [
    "### RAG Pipelines - Data Ingestion to Vector DB pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d3f6104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2c63d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 PDF files to process\n",
      "\n",
      "Processing: attention.pdf\n",
      "✔️ Loaded 1 pages\n",
      "\n",
      "Processing: embeddings.pdf\n",
      "✔️ Loaded 1 pages\n",
      "\n",
      "Processing: objectdetection.pdf\n",
      "✔️ Loaded 1 pages\n",
      "\n",
      " Total Documents Loaded: 3\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdfs inside the directory\n",
    "\n",
    "\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all pdf files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "\n",
    "    ## find all pdf files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "\n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "\n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            # add the source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata[\"source_file\"] = pdf_file.name\n",
    "                doc.metadata[\"file_type\"] = \"pdf\"\n",
    "\n",
    "            all_documents.extend(documents)\n",
    "            print(f\"✔️ Loaded {len(documents)} pages\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error: {e} pages\")\n",
    "\n",
    "    print(f\"\\n Total Documents Loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "\n",
    "## process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "147bdd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-07T13:18:48+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-07T13:18:48+00:00', 'subject': '(unspecified)', 'title': 'Attention in Machine Learning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention in Machine Learning\\nAttention in Machine Learning is a concept that allows models to focus on the most\\nrelevant parts of the input data when making predictions. It was first introduced in\\nthe field of Natural Language Processing (NLP) and has become a key idea behind\\nmany modern AI models such as Transformers, BERT, and GPT.\\nIn traditional neural networks, all input data is treated equally, which can make it\\ndifficult for models to capture long-term dependencies. Attention solves this by\\nassigning different “weights” to different parts of the input, meaning the model can\\ndecide which words, pixels, or features are most important for the current task.\\nThe mechanism works by computing a score for each input element based on its\\nrelevance to the output being generated. These scores are then converted into\\nprobabilities using a softmax function. The model uses these probabilities to focus\\nmore on important elements while ignoring less relevant ones.\\nThere are different types of attention mechanisms, including: \\x7f Additive Attention –\\ncompares the query and key using a feed-forward network. \\x7f Dot-Product\\n(Multiplicative) Attention – uses a dot product to measure similarity. \\x7f Self-Attention\\n– allows a sequence to look at itself to capture relationships between its elements.\\nThis is the foundation of the Transformer model.\\nIn summary, Attention helps models understand context better by dynamically\\nfocusing on relevant information. It has revolutionized how machines process\\nlanguage, images, and even videos, making it one of the most powerful ideas in\\nmodern Artificial Intelligence.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-07T13:20:25+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-07T13:20:25+00:00', 'subject': '(unspecified)', 'title': 'Embeddings in Machine Learning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Embeddings in Machine Learning\\nEmbeddings in Machine Learning are a way to represent complex data (like words,\\nimages, or items) as numerical vectors in a continuous space. This transformation\\nallows algorithms to understand similarities and relationships between data points\\nmore effectively.\\nMost machine learning models work with numbers, not raw text or images.\\nEmbeddings solve this problem by converting high-dimensional or symbolic data\\ninto lower-dimensional numeric form. This helps models learn patterns, context,\\nand relationships between data points.\\nAn embedding maps each item (for example, a word) to a vector of real numbers.\\nThe goal is for similar items to have vectors that are close together in the\\nembedding space. For example, in a word embedding model, the vectors for “king”\\nand “queen” will be close because they share similar meanings.\\nCommon types of embeddings include: \\x7f Word Embeddings – Represent words as\\nvectors (e.g., Word2Vec, GloVe, FastText). \\x7f Sentence or Document Embeddings\\n– Represent larger text units (e.g., Sentence-BERT, Universal Sentence Encoder).\\n\\x7f Image Embeddings – Represent visual data (used in computer vision tasks). \\x7f\\nGraph Embeddings – Represent nodes in graphs while preserving relationships.\\nEmbeddings are widely used in applications like recommendation systems, search\\nengines, natural language processing, and image recognition. They allow models\\nto understand semantic meaning, similarity, and context beyond simple keyword\\nmatching.\\nIn summary, embeddings are the foundation of many modern AI systems. They\\nenable machines to represent and reason about data in a meaningful way, bridging\\nthe gap between raw information and intelligent understanding.'),\n",
       " Document(metadata={'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-07T13:21:17+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-07T13:21:17+00:00', 'subject': '(unspecified)', 'title': 'Object Detection in Machine Learning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Object Detection in Machine Learning\\nObject Detection is a computer vision technique that allows machines to identify\\nand locate multiple objects within an image or video. It not only tells what objects\\nare present but also where they are located using bounding boxes.\\nUnlike simple image classification, which only identifies the category of an image,\\nobject detection provides spatial information by drawing boxes around each\\ndetected object. This makes it useful for real-world applications like autonomous\\nvehicles, surveillance, and medical imaging.\\nObject detection models usually work in two steps: first, they extract important\\nfeatures from the image, and then they classify and localize objects. Modern deep\\nlearning models use convolutional neural networks (CNNs) to perform both steps\\nefficiently.\\nPopular object detection architectures include: \\x7f R-CNN and Fast R-CNN – Use\\nregion proposals followed by CNN-based classification. \\x7f YOLO (You Only Look\\nOnce) – Performs detection in a single pass, making it very fast. \\x7f SSD (Single\\nShot MultiBox Detector) – Balances speed and accuracy using multiple feature\\nmaps. \\x7f Faster R-CNN – Combines region proposal and detection into one network\\nfor higher efficiency.\\nApplications of object detection include: \\x7f Self-driving cars (detecting pedestrians,\\ntraffic signs, vehicles) \\x7f Security systems (face and motion detection) \\x7f Healthcare\\n(detecting tumors or medical anomalies) \\x7f Retail (inventory monitoring and shelf\\nanalysis) \\x7f Robotics (object localization and manipulation)\\nIn summary, Object Detection combines the power of image classification and\\nlocalization, enabling machines to visually understand and interact with their\\nenvironment. It is a cornerstone technology in computer vision and continues to\\nevolve with more efficient and accurate models.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8787f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text Splitting get into chunks\n",
    "def split_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\" \"Split docuemnts into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"\\nSplit {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "\n",
    "    ## show example of chunk\n",
    "    if split_docs:\n",
    "        print(\"\\nExample chunk:\")\n",
    "        print(f\"Content: \\n{split_docs[0].page_content}\\n---\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c1c59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split 3 documents into 7 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: \n",
      "Attention in Machine Learning\n",
      "Attention in Machine Learning is a concept that allows models to focus on the most\n",
      "relevant parts of the input data when making predictions. It was first introduced in\n",
      "the field of Natural Language Processing (NLP) and has become a key idea behind\n",
      "many modern AI models such as Transformers, BERT, and GPT.\n",
      "In traditional neural networks, all input data is treated equally, which can make it\n",
      "difficult for models to capture long-term dependencies. Attention solves this by\n",
      "assigning different “weights” to different parts of the input, meaning the model can\n",
      "decide which words, pixels, or features are most important for the current task.\n",
      "The mechanism works by computing a score for each input element based on its\n",
      "relevance to the output being generated. These scores are then converted into\n",
      "probabilities using a softmax function. The model uses these probabilities to focus\n",
      "more on important elements while ignoring less relevant ones.\n",
      "---\n",
      "Metadata: {'producer': 'ReportLab PDF Library - www.reportlab.com', 'creator': '(unspecified)', 'creationdate': '2025-10-07T13:18:48+00:00', 'author': '(anonymous)', 'keywords': '', 'moddate': '2025-10-07T13:18:48+00:00', 'subject': '(unspecified)', 'title': 'Attention in Machine Learning', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_documents(all_pdf_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43a8528",
   "metadata": {},
   "source": [
    "### Embedding and Vector Store DB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95dc5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc42b265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Model loaded successfully. Embedding dimension: 384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x15289388750>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding generation using Sentence Transformer\"\"\"\n",
    "\n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialzes the embedding manager\n",
    "\n",
    "        Args:\n",
    "            model_name: HuggingFace model name for sentence embeddings\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "\n",
    "    def _load_model(self):\n",
    "        \"\"\"Load the sentence Tranformer model\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model: {self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(\n",
    "                f\"Model loaded successfully. Embedding dimension: {self.model.get_sentence_embedding_dimension()}\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {self.model_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def generate_embeddings(self, texts: List[str]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "\n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "\n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not found\")\n",
    "\n",
    "        print(f\"Generating embeddings for {len(texts) } texts...\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape: {embeddings.shape}\")\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "## initialize the embedding manager\n",
    "\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12158e1b",
   "metadata": {},
   "source": [
    "### Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f89ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector Store initialzed. Colleciton: pdf_documents\n",
      "Existing documents in collection: 14\n"
     ]
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a chromaDB vector Store\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        collection_name: str = \"pdf_documents\",\n",
    "        persist_directory: str = \"../data/vector_store\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "\n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChormaDB client and connection\"\"\"\n",
    "        try:\n",
    "            # create persistent ChromaDb client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "\n",
    "            # Get or create Collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"},\n",
    "            )\n",
    "\n",
    "            print(f\"Vector Store initialzed. Colleciton: {self.collection.name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "\n",
    "        Args:\n",
    "            docuements: List of langchain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "\n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "\n",
    "        ## prepare data for chroma db\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embedding_list = []\n",
    "\n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique id\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_i\"\n",
    "            ids.append(doc_id)\n",
    "\n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata[\"doc_index\"] = i\n",
    "            metadata[\"content_length\"] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "\n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "\n",
    "            # Embedding\n",
    "            embedding_list.append(embedding.tolist())\n",
    "\n",
    "        # Add Coolection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embedding_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text,\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "vector_store = VectorStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136001f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for 7 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (7, 384)\n",
      "Adding 7 documents to vector store...\n",
      "Successfully added 7 documents to vector store\n",
      "Total documents in collection: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Conver the text to embeddings and add to vector store\n",
    "texts = [doc.page_content for doc in chunks]\n",
    "\n",
    "## Generate the embeddings\n",
    "embeddings = embedding_manager.generate_embeddings(texts)\n",
    "\n",
    "## store in the vector database\n",
    "vector_store.add_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b23a7f",
   "metadata": {},
   "source": [
    "### Retrievar Pipeline From Vector Store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e2fa39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    def __init__(self,vector_store: VectorStore,embedding_manager:EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Iniitialze the retriever\n",
    "\n",
    "        Args:\n",
    "            vector_store: Vector Store containing document embeddings\n",
    "            embeddgin_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self,query:str, top_k:int = 5, score_threshold: float = 0.0)->List[Dict[str,Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for query\n",
    "\n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "\n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata \n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Tok k: {top_k}, score_threshold: {score_threshold}\")\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "\n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "\n",
    "            # Process results\n",
    "\n",
    "            retrieved_docs = []\n",
    "\n",
    "            if results.get('documents') and results['documents'] and results['documents'][0] and results.get('metadatas') and results['metadatas'] and results['metadatas'][0] and results.get(\"distances\") and results['distances']:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "\n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids,documents,metadatas,distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "\n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            \"id\":doc_id,\n",
    "                            \"content\":document,\n",
    "                            \"metadata\": metadata,\n",
    "                            \"similarity_score\":similarity_score,\n",
    "                            \"distance\":distance,\n",
    "                            \"rank\":i + 1                            \n",
    "                        })\n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "\n",
    "rag_retriever = RAGRetriever(vector_store,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad410e2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1528950db90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "27034ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is attention in machine learning'\n",
      "Tok k: 5, score_threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 35.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_ad8d2538_i',\n",
       "  'content': 'Attention in Machine Learning\\nAttention in Machine Learning is a concept that allows models to focus on the most\\nrelevant parts of the input data when making predictions. It was first introduced in\\nthe field of Natural Language Processing (NLP) and has become a key idea behind\\nmany modern AI models such as Transformers, BERT, and GPT.\\nIn traditional neural networks, all input data is treated equally, which can make it\\ndifficult for models to capture long-term dependencies. Attention solves this by\\nassigning different “weights” to different parts of the input, meaning the model can\\ndecide which words, pixels, or features are most important for the current task.\\nThe mechanism works by computing a score for each input element based on its\\nrelevance to the output being generated. These scores are then converted into\\nprobabilities using a softmax function. The model uses these probabilities to focus\\nmore on important elements while ignoring less relevant ones.',\n",
       "  'metadata': {'page': 0,\n",
       "   'doc_index': 0,\n",
       "   'title': 'Attention in Machine Learning',\n",
       "   'subject': '(unspecified)',\n",
       "   'file_type': 'pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'author': '(anonymous)',\n",
       "   'total_pages': 1,\n",
       "   'trapped': '/False',\n",
       "   'content_length': 969,\n",
       "   'page_label': '1',\n",
       "   'producer': 'ReportLab PDF Library - www.reportlab.com',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'keywords': '',\n",
       "   'creationdate': '2025-10-07T13:18:48+00:00',\n",
       "   'creator': '(unspecified)',\n",
       "   'moddate': '2025-10-07T13:18:48+00:00'},\n",
       "  'similarity_score': 0.5486676692962646,\n",
       "  'distance': 0.45133233070373535,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_7a2832a3_i',\n",
       "  'content': 'Attention in Machine Learning\\nAttention in Machine Learning is a concept that allows models to focus on the most\\nrelevant parts of the input data when making predictions. It was first introduced in\\nthe field of Natural Language Processing (NLP) and has become a key idea behind\\nmany modern AI models such as Transformers, BERT, and GPT.\\nIn traditional neural networks, all input data is treated equally, which can make it\\ndifficult for models to capture long-term dependencies. Attention solves this by\\nassigning different “weights” to different parts of the input, meaning the model can\\ndecide which words, pixels, or features are most important for the current task.\\nThe mechanism works by computing a score for each input element based on its\\nrelevance to the output being generated. These scores are then converted into\\nprobabilities using a softmax function. The model uses these probabilities to focus\\nmore on important elements while ignoring less relevant ones.',\n",
       "  'metadata': {'trapped': '/False',\n",
       "   'doc_index': 0,\n",
       "   'producer': 'ReportLab PDF Library - www.reportlab.com',\n",
       "   'file_type': 'pdf',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'moddate': '2025-10-07T13:18:48+00:00',\n",
       "   'total_pages': 1,\n",
       "   'content_length': 969,\n",
       "   'creationdate': '2025-10-07T13:18:48+00:00',\n",
       "   'title': 'Attention in Machine Learning',\n",
       "   'author': '(anonymous)',\n",
       "   'page': 0,\n",
       "   'page_label': '1',\n",
       "   'keywords': '',\n",
       "   'creator': '(unspecified)',\n",
       "   'subject': '(unspecified)'},\n",
       "  'similarity_score': 0.5486676692962646,\n",
       "  'distance': 0.45133233070373535,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_82b290e2_i',\n",
       "  'content': 'Attention in Machine Learning\\nAttention in Machine Learning is a concept that allows models to focus on the most\\nrelevant parts of the input data when making predictions. It was first introduced in\\nthe field of Natural Language Processing (NLP) and has become a key idea behind\\nmany modern AI models such as Transformers, BERT, and GPT.\\nIn traditional neural networks, all input data is treated equally, which can make it\\ndifficult for models to capture long-term dependencies. Attention solves this by\\nassigning different “weights” to different parts of the input, meaning the model can\\ndecide which words, pixels, or features are most important for the current task.\\nThe mechanism works by computing a score for each input element based on its\\nrelevance to the output being generated. These scores are then converted into\\nprobabilities using a softmax function. The model uses these probabilities to focus\\nmore on important elements while ignoring less relevant ones.',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'page_label': '1',\n",
       "   'total_pages': 1,\n",
       "   'title': 'Attention in Machine Learning',\n",
       "   'moddate': '2025-10-07T13:18:48+00:00',\n",
       "   'trapped': '/False',\n",
       "   'creationdate': '2025-10-07T13:18:48+00:00',\n",
       "   'content_length': 969,\n",
       "   'doc_index': 0,\n",
       "   'page': 0,\n",
       "   'keywords': '',\n",
       "   'producer': 'ReportLab PDF Library - www.reportlab.com',\n",
       "   'subject': '(unspecified)',\n",
       "   'creator': '(unspecified)',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'author': '(anonymous)'},\n",
       "  'similarity_score': 0.5486676692962646,\n",
       "  'distance': 0.45133233070373535,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_72700b37_i',\n",
       "  'content': 'probabilities using a softmax function. The model uses these probabilities to focus\\nmore on important elements while ignoring less relevant ones.\\nThere are different types of attention mechanisms, including: \\x7f Additive Attention –\\ncompares the query and key using a feed-forward network. \\x7f Dot-Product\\n(Multiplicative) Attention – uses a dot product to measure similarity. \\x7f Self-Attention\\n– allows a sequence to look at itself to capture relationships between its elements.\\nThis is the foundation of the Transformer model.\\nIn summary, Attention helps models understand context better by dynamically\\nfocusing on relevant information. It has revolutionized how machines process\\nlanguage, images, and even videos, making it one of the most powerful ideas in\\nmodern Artificial Intelligence.',\n",
       "  'metadata': {'author': '(anonymous)',\n",
       "   'page': 0,\n",
       "   'source_file': 'attention.pdf',\n",
       "   'creator': '(unspecified)',\n",
       "   'keywords': '',\n",
       "   'page_label': '1',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'moddate': '2025-10-07T13:18:48+00:00',\n",
       "   'trapped': '/False',\n",
       "   'total_pages': 1,\n",
       "   'subject': '(unspecified)',\n",
       "   'creationdate': '2025-10-07T13:18:48+00:00',\n",
       "   'doc_index': 1,\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'ReportLab PDF Library - www.reportlab.com',\n",
       "   'title': 'Attention in Machine Learning',\n",
       "   'content_length': 787},\n",
       "  'similarity_score': 0.34871023893356323,\n",
       "  'distance': 0.6512897610664368,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_886c3a14_i',\n",
       "  'content': 'probabilities using a softmax function. The model uses these probabilities to focus\\nmore on important elements while ignoring less relevant ones.\\nThere are different types of attention mechanisms, including: \\x7f Additive Attention –\\ncompares the query and key using a feed-forward network. \\x7f Dot-Product\\n(Multiplicative) Attention – uses a dot product to measure similarity. \\x7f Self-Attention\\n– allows a sequence to look at itself to capture relationships between its elements.\\nThis is the foundation of the Transformer model.\\nIn summary, Attention helps models understand context better by dynamically\\nfocusing on relevant information. It has revolutionized how machines process\\nlanguage, images, and even videos, making it one of the most powerful ideas in\\nmodern Artificial Intelligence.',\n",
       "  'metadata': {'content_length': 787,\n",
       "   'page_label': '1',\n",
       "   'title': 'Attention in Machine Learning',\n",
       "   'total_pages': 1,\n",
       "   'creator': '(unspecified)',\n",
       "   'page': 0,\n",
       "   'subject': '(unspecified)',\n",
       "   'creationdate': '2025-10-07T13:18:48+00:00',\n",
       "   'source_file': 'attention.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'keywords': '',\n",
       "   'trapped': '/False',\n",
       "   'producer': 'ReportLab PDF Library - www.reportlab.com',\n",
       "   'author': '(anonymous)',\n",
       "   'source': '..\\\\data\\\\pdf\\\\attention.pdf',\n",
       "   'moddate': '2025-10-07T13:18:48+00:00',\n",
       "   'doc_index': 1},\n",
       "  'similarity_score': 0.34871023893356323,\n",
       "  'distance': 0.6512897610664368,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"what is attention in machine learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45719883",
   "metadata": {},
   "source": [
    "### Integration VectorDB Context Pipeline with LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d4dc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import SecretStr\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "### Initialize the GROQ LLM\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if groq_api_key is None:\n",
    "    raise ValueError(\"GROQ_API_KEY environment variable not set.\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    api_key=SecretStr(groq_api_key),\n",
    "    model=\"gemma2-9b-it\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "\n",
    "### Simple RAG function: retrieve context + generate response\n",
    "def rag_simple(query, retriever: RAGRetriever, llm: ChatGroq, top_k=3):\n",
    "    # retrieve the context\n",
    "    results = retriever.retrieve(query=query, top_k=top_k)\n",
    "    context = \"\\n\\n\".join([doc[\"content\"] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question\"\n",
    "\n",
    "    # generate the answer using GROQ LLM\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely\n",
    "\t\t\tContext:\n",
    "\t\t\t{context}\n",
    "\n",
    "\t\t\tQuestion: {query}\n",
    "\n",
    "\t\t\tAnswer:\"\"\"\n",
    "\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b6fb7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'attention in machine learning'\n",
      "Tok k: 3, score_threshold: 0.0\n",
      "Generating embeddings for 1 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 43.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape: (1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention in machine learning allows models to focus on the most relevant parts of input data, improving their ability to capture long-term dependencies and make more accurate predictions.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = rag_simple(\"attention in machine learning\",retriever=rag_retriever,llm=llm,top_k=3)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20429e0f",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d88ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Enhanced RAG Pipeline features ---\n",
    "\n",
    "\n",
    "def rag_advance(\n",
    "    query: str,\n",
    "    retriever: RAGRetriever,\n",
    "    llm: ChatGroq,\n",
    "    top_k=5,\n",
    "    min_score=0.2,\n",
    "    return_context=False,\n",
    "):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full text.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {\n",
    "            \"answer\": \"No relevant context found.\",\n",
    "            \"sources\": [],\n",
    "            \"confidence\": 0.0,\n",
    "            \"context\": \"\",\n",
    "        }\n",
    "\n",
    "    # prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc[\"content\"] for doc in results])\n",
    "    sources = [\n",
    "        {\n",
    "            \"source\": doc[\"metadata\"].get(\n",
    "                \"source_file\", doc[\"metadata\"].get(\"source\", \"unknown\")\n",
    "            ),\n",
    "            \"page\": doc[\"metadata\"].get(\"page\", \"unknown\"),\n",
    "            \"score\": doc[\"similarity_score\"],\n",
    "            \"preview\": doc[\"content\"][120:] + \"...\",\n",
    "        }\n",
    "        for doc in results\n",
    "    ]\n",
    "    \n",
    "    confidence = max(doc[\"similarity_score\"] for doc in results)\n",
    "\n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the qeustion concisely \\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context,query=query)])\n",
    "\n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence':confidence\n",
    "    }\n",
    "\n",
    "    if return_context:\n",
    "        output[\"context\"] = context\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c847f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example Usage\n",
    "result = rag_advance(\"what is the attention mechanism\",rag_retriever,llm,top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result[\"answer\"])\n",
    "print(\"Sources\", result[\"sources\"]) \n",
    "print(\"Cofidence\", result[\"confidence\"]) \n",
    "print(\"Context Preview\", result[\"context\"])     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
